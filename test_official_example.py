"""
ä½¿ç”¨å®˜æ–¹ç¤ºä¾‹ä»£ç ç›´æ¥æµ‹è¯• DeepSeek-OCR
"""
from transformers import AutoModel, AutoTokenizer
import torch
import os

os.environ["CUDA_VISIBLE_DEVICES"] = '0'
# ç¦ç”¨ JIT ç¼–è¯‘
os.environ["PYTORCH_JIT"] = "0"
os.environ["PYTORCH_NVFUSER_DISABLE"] = "1"
os.environ["TORCH_COMPILE_DISABLE"] = "1"

model_name = 'deepseek-ai/DeepSeek-OCR'

print("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModel.from_pretrained(
    model_name, 
    attn_implementation='eager',  # ä½¿ç”¨ eager æ¨¡å¼
    trust_remote_code=True, 
    use_safetensors=True
)

print("ğŸ“¦ æ¨¡å‹åŠ è½½å®Œæˆ,æ­£åœ¨ç§»åˆ° GPU...")
model = model.eval().cuda().to(torch.bfloat16)

print("âœ… æ¨¡å‹å‡†å¤‡å®Œæˆ! (GPU æ¨¡å¼)")

prompt = "<image>\n<|grounding|>Convert the document to markdown."
image_file = 'test_image.jpg'
output_path = './test_output'

print(f"\nğŸ“„ å¼€å§‹ OCR è¯†åˆ«: {image_file}")
print(f"ğŸ’¡ æç¤ºè¯: {prompt}")

try:
    res = model.infer(
        tokenizer, 
        prompt=prompt, 
        image_file=image_file, 
        output_path=output_path, 
        base_size=1024, 
        image_size=640, 
        crop_mode=True, 
        save_results=True, 
        test_compress=True,
        eval_mode=True  # ä½¿ç”¨ eval æ¨¡å¼,ä¸ä½¿ç”¨ streamer
    )
    
    print("\nâœ… OCR è¯†åˆ«å®Œæˆ!")
    print(f"\nğŸ“ è¯†åˆ«ç»“æœ:\n{res}")
    
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()

